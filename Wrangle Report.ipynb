{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Wrangle Report</center>\n",
    "\n",
    "## <center>June 2019</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Introduction\n",
    "This wrangle report is about data cleaning of dogtweet data. WeRateDogs is a Twitter account that rates people's dogs with a humorous comment about the dog. It comes with a special rating system including a numerator and denominator which is normally 10.  This wrangling effort include three parts below:\n",
    "1. Data Gathering\n",
    "2. Data Assess\n",
    "3. Data Cleaning\n",
    "\n",
    "The text content is about WeRateDods' comment on each dog. The raw data include three parts:\n",
    "1. Twitter_archive_enhanced: this is provided csv data file. it  contains basic tweet data for all 5000+ of their tweets, but not everything. \n",
    "2. Image prediction file: it is a table full of image predictions (the top three only) alongside each tweet ID, image URL, and the image number that corresponded to the most confident prediction (numbered 1 to 4 since tweets can have up to four images).\n",
    "3. addtional data via Twitter API: these are valuable retweet and favorite tweet count frow twitter API. Code in wrangle_act will show how to get these data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Data Gathering\n",
    "\n",
    "The raw data include three parts:\n",
    "\n",
    "1. Twitter_archive_enhanced: this is provided csv data file. it  contains basic tweet data for all 5000+ of their tweets, but not everything. \n",
    "2. Image prediction file: it is a table full of image predictions (the top three only) alongside each tweet ID, image URL, and the image number that corresponded to the most confident prediction (numbered 1 to 4 since tweets can have up to four images).\n",
    "3. addtional data via Twitter API: these are valuable retweet and favorite tweet count frow twitter API. Code in wrangle_act will show how to get these data.\n",
    "\n",
    "The first two datasets are stored in csv and could be downloaded and converted pandas data frame.  The last data need twitter API query to store data file.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Data Assess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After gather above three data sets, data assess is performed visually and programatically to identify quality and tidyness of datasets.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I. Data Quality Issue\n",
    "\n",
    "Quality issues:\n",
    "\n",
    "For twitter_archive data frame\n",
    "\n",
    "1. tweet_id column should be string type instead of int type since we are not going to perform any calculation on it. This should be updated for all three columns\n",
    "2. timestamp column should be in datetime type instead of object\n",
    "3. The following columns have over 90% of missing values, which should be inspected and cleaned: \"in_reply_to_status_id\", \"in_reply_to_user_id\", \"retweeted_status_id\", \"retweeted_status_user_id\", \"retweeted_status_timestamp\"\n",
    "3. The source column contain < and http string. The format is not quite readable. it should be cleand by 4 types of sources\n",
    "4. Current rows of denominator 0, 11, 50 â€™s values are wrong. The previous rating is cited.\n",
    "5. The dog name is extracted after \" this is \". There are some errors that \"a\", \"an\", \"the\" are cited. They shoudl be updated\n",
    "6. Some expanded_urls are with missing values, which could be filled programatically following same pattern.\n",
    "7. the rows with retweet status not null should be removed since we only analyze the original tweet\n",
    "\n",
    "For image_df data frame\n",
    "\n",
    "1. for the rows with all three prediction (p1_dog, p2_dog, p3_dog) as False, they should be removed from this data analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### II. Data Tidiness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. dog_stage should be one column instead of 4 separate columns, which could be cleaned\n",
    "2. dog names should all be converted into lower case\n",
    "3. Some expanded_urls column have repeated url value with \",\" as separator, which should be cleaned\n",
    "4. Merge all three dataframes into one data table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "It is important to clean up data prior to perform exploratory analysis and extracting insifhts. Quality of data coudl severely impact on the final results. Wrangling process could be loop back and forth. However, this is a necessary effort."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
